{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def gradient_penalty(X,X_fake,epsilon):\n",
    "    inputs_gp = epsilon*X + ((1-epsilon)*X_fake)\n",
    "    disc_inputs_gp = discriminator(inputs_gp,reuse=True)\n",
    "    grad_disc_inputs = tf.gradients(disc_inputs_gp,inputs_gp)\n",
    "    penalty = tf.square(tf.norm(grad_disc_inputs)-1)\n",
    "    return penalty\n",
    "    \n",
    "def data():   \n",
    "    scale = 2.\n",
    "    centers = [\n",
    "        (1,0),\n",
    "        (-1,0),\n",
    "        (0,1),\n",
    "        (0,-1),\n",
    "        (1./np.sqrt(2), 1./np.sqrt(2)),\n",
    "        (1./np.sqrt(2), -1./np.sqrt(2)),\n",
    "        (-1./np.sqrt(2), 1./np.sqrt(2)),\n",
    "        (-1./np.sqrt(2), -1./np.sqrt(2))\n",
    "    ]\n",
    "    centers = [(scale*x,scale*y) for x,y in centers]\n",
    "    while True:\n",
    "        dataset = []\n",
    "        for i in range(batch_size):\n",
    "            point = np.random.randn(2)*.02\n",
    "            center = random.choice(centers)\n",
    "            point[0] += center[0]\n",
    "            point[1] += center[1]\n",
    "            dataset.append(point)\n",
    "        dataset = np.array(dataset, dtype='float32')\n",
    "        dataset /= 1.414 # stdev\n",
    "        yield dataset\n",
    "\n",
    "\n",
    "\n",
    "def generate_image(true_dist, samples, disc_map, Range = 3, N = 128):\n",
    "            plt.clf()\n",
    "            x = y = np.linspace(-Range, Range, N)\n",
    "            plt.contour(x,y,disc_map.reshape((len(x), len(y))).transpose())\n",
    "            plt.scatter(true_dist[:, 0], true_dist[:, 1], c='orange',  marker='+')\n",
    "            plt.scatter(samples[:, 0],    samples[:, 1],    c='green', marker='+')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(Z, output_dim = 1, h_dim = 32, reuse = False):\n",
    "    with tf.variable_scope('generator',reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(Z,h_dim)\n",
    "        hidden1 = tf.nn.relu(hidden1)\n",
    "        \n",
    "        hidden2 = tf.layers.dense(hidden1,h_dim)\n",
    "        hidden2 = tf.nn.relu(hidden2)\n",
    "        \n",
    "        hidden3 = tf.layers.dense(hidden2,h_dim)\n",
    "        hidden3 = tf.nn.relu(hidden3)\n",
    "        \n",
    "        output = tf.layers.dense(hidden3,output_dim)\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X, h_dim = 512, reuse = False):\n",
    "    with tf.variable_scope('discriminator',reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(X,h_dim)\n",
    "        hidden1 = tf.nn.relu(hidden1)\n",
    "        \n",
    "        hidden2 = tf.layers.dense(hidden1,h_dim)\n",
    "        hidden2 = tf.nn.relu(hidden2)\n",
    "        \n",
    "        hidden3 = tf.layers.dense(hidden2,h_dim)\n",
    "        hidden3 = tf.nn.relu(hidden3)\n",
    "        \n",
    "        logits = tf.layers.dense(hidden3,1)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(logits_fake):\n",
    "    return -tf.reduce_mean(logits_fake)\n",
    "\n",
    "def discriminator_loss(logits_real, logits_fake, penalty, lambd = 10):\n",
    "    return tf.reduce_mean(logits_fake) - tf.reduce_mean(logits_real) + lambd*tf.reduce_mean(penalty) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_op(disc_loss, gen_loss, learning_rate = 1e-4, beta1 = 0.0, beta2 = 0.9):\n",
    "    disc_var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='discriminator')\n",
    "    gen_var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='generator')\n",
    "    \n",
    "    disc_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1,beta2=beta2)\n",
    "    gen_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1,beta2=beta2)\n",
    "    \n",
    "    disc_train_op = disc_optimizer.minimize(disc_loss,var_list=disc_var_list)\n",
    "    gen_train_op = disc_optimizer.minimize(gen_loss,var_list=gen_var_list)\n",
    "    return disc_train_op,gen_train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(z_dim = 2, output_dim = 1, batch_size = 1024):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape = [None, output_dim])\n",
    "    Z = tf.random_normal([2, z_dim])\n",
    "    Z_ = tf.random_uniform([batch_size,z_dim])\n",
    "    return X,Z,Z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparmetrs\n",
    "h_dim = 512\n",
    "z_dim = 2\n",
    "output_dim = 2\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "beta1 = 0.0\n",
    "beta2 = 0.9\n",
    "lambd = 10\n",
    "epochs = 5001\n",
    "batch_it = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tf.Graph()\n",
    "with g1.as_default() as graph:\n",
    "    X,Z,Z_ = input_data(z_dim, output_dim, batch_size)\n",
    "    X_fake = generator(Z, output_dim=output_dim, h_dim=h_dim)\n",
    "    X_fake_ = generator(Z_, output_dim=output_dim, h_dim=h_dim, reuse=True)\n",
    "    epsilon = tf.random_uniform([batch_size,1],minval=0,maxval=1)\n",
    "\n",
    "    logits_real = discriminator(X,h_dim=h_dim)\n",
    "    logits_fake = discriminator(X_fake,h_dim=h_dim,reuse=True)\n",
    "    penalty = gradient_penalty(X,X_fake_,epsilon)\n",
    "\n",
    "    disc_loss = discriminator_loss(logits_real,logits_fake,penalty,lambd)\n",
    "    gen_loss = generator_loss(logits_fake)\n",
    "\n",
    "    disc_train_op,gen_train_op = training_op(disc_loss, gen_loss, learning_rate=learning_rate, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "Range = 3\n",
    "\n",
    "points = np.zeros((N, N, 2), dtype='float32')\n",
    "points[:,:,0] = np.linspace(-Range, Range, N)[:,None]\n",
    "points[:,:,1] = np.linspace(-Range, Range, N)[None,:]\n",
    "points = points.reshape((-1,2))\n",
    "print(points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    data_gen = data()\n",
    "    gen_cost = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        if epoch > 0:\n",
    "            _ = sess.run(gen_train_op)         \n",
    "        for i in range(batch_it):\n",
    "            d = next(data_gen)\n",
    "            feed_dict = {X:d}\n",
    "            disc_cost,gen_cost,_ = sess.run([disc_loss,gen_loss,disc_train_op],feed_dict={X:d})\n",
    "        if epoch %50 == 0:\n",
    "            x_fake = sess.run(X_fake)\n",
    "            print(\"====\")\n",
    "            print(\"Iteration: {}/{}\".format(epoch,epochs-1))\n",
    "            print(\"Discriminator loss: {}\".format(disc_cost))\n",
    "            print(\"Generator loss: {}\".format(gen_cost))\n",
    "        if epoch %100 ==0:\n",
    "            samples, disc_map = sess.run([X_fake, logits_real], feed_dict={X:points})\n",
    "            generate_image(d, samples, disc_map, Range=Range, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
