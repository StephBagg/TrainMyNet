{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def norm_pdf(x, mu, sig):\n",
    "    #Returns the PDF of Gaussian distribution (for plotting)\n",
    "    #with mean=mu\n",
    "    #and standard deviation = sig\n",
    "    p = (1.0/np.sqrt(2*np.pi*sig**2))*np.exp(-(x-mu)**2/(2*sig**2))\n",
    "    return p\n",
    "\n",
    "def data(mu1 = -1.0, mu2 = 2.0, sig1 = 0.5, sig2 = 0.6, batch_size = 1024):\n",
    "    #Multimodel distribution\n",
    "    s1 = np.random.normal(mu1,sig1,[batch_size//2,1])\n",
    "    s2 = np.random.normal(mu2,sig2,[batch_size//2,1])\n",
    "    d = np.concatenate((s1,s2),axis=0).reshape([batch_size,1])\n",
    "    np.random.shuffle(d)\n",
    "    return d\n",
    "\n",
    "def generate_image(iteration, x_fake, mu1 = -1.0, mu2 = 2.0, sig1 = 0.5, sig2 = 0.6):\n",
    "            xplot = np.linspace(-4,4,1000)\n",
    "            p1 = norm_pdf(xplot,mu1,sig1)\n",
    "            p2 = norm_pdf(xplot,mu2,sig2) \n",
    "            p = p1/2+p2/2\n",
    "        \n",
    "            plt.clf()\n",
    "            plt.hist(x_fake,100,density=True)\n",
    "            plt.plot(xplot,p,'g-')\n",
    "            plt.xlim(-4,4)\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"Density\")\n",
    "            plt.title(\"Step = {}\".format(iteration))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(Z, output_dim = 1, h_dim = 32, reuse = False):\n",
    "    with tf.variable_scope('generator',reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(Z,h_dim)\n",
    "        hidden1 = tf.nn.relu(hidden1)\n",
    "        \n",
    "        hidden2 = tf.layers.dense(hidden1,h_dim)\n",
    "        hidden2 = tf.nn.relu(hidden2)\n",
    "        \n",
    "        hidden3 = tf.layers.dense(hidden2,h_dim)\n",
    "        hidden3 = tf.nn.relu(hidden3)\n",
    "        \n",
    "        output = tf.layers.dense(hidden3,output_dim)\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X, h_dim = 32, reuse = False):\n",
    "    with tf.variable_scope('discriminator',reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(X,h_dim)\n",
    "        hidden1 = tf.nn.relu(hidden1)\n",
    "        \n",
    "        hidden2 = tf.layers.dense(hidden1,h_dim)\n",
    "        hidden2 = tf.nn.relu(hidden2)\n",
    "        \n",
    "        hidden3 = tf.layers.dense(hidden2,h_dim)\n",
    "        hidden3 = tf.nn.relu(hidden3)\n",
    "        \n",
    "        logits = tf.layers.dense(hidden3,1)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(logits_fake):\n",
    "    labels = tf.ones_like(logits_fake)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits_fake)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def discriminator_loss(logits_real, logits_fake):\n",
    "    labels_real = tf.ones_like(logits_real)\n",
    "    loss_real = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_real, logits=logits_real)\n",
    "    \n",
    "    labels_fake = tf.zeros_like(logits_fake)\n",
    "    loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_fake, logits=logits_fake)\n",
    "    return tf.reduce_mean(loss_real) + tf.reduce_mean(loss_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_op(disc_loss, gen_loss, learning_rate = 1e-4, beta1 = 0.0, beta2 = 0.9):\n",
    "    disc_var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='discriminator')\n",
    "    gen_var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='generator')\n",
    "    \n",
    "    disc_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1,beta2=beta2)\n",
    "    gen_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1,beta2=beta2)\n",
    "    \n",
    "    disc_train_op = disc_optimizer.minimize(disc_loss,var_list=disc_var_list)\n",
    "    gen_train_op = disc_optimizer.minimize(gen_loss,var_list=gen_var_list)\n",
    "    return disc_train_op,gen_train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(z_dim = 2, output_dim = 1, batch_size = 1024):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape = [batch_size, output_dim])\n",
    "    Z = tf.random_normal([batch_size, z_dim])\n",
    "    return X,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparmetrs\n",
    "h_dim = 32\n",
    "z_dim = 2\n",
    "output_dim = 1\n",
    "batch_size = 1024\n",
    "learning_rate = 1e-4\n",
    "beta1 = 0.0\n",
    "beta2 = 0.9\n",
    "epochs = 201\n",
    "batch_it = 10\n",
    "\n",
    "#distribution parameters\n",
    "mu1 = -1.0\n",
    "mu2 = 2.0  \n",
    "sig1 = 0.5\n",
    "sig2 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tf.Graph()\n",
    "with g1.as_default() as graph:\n",
    "    X,Z = input_data(z_dim, output_dim, batch_size)\n",
    "    X_fake = generator(Z, output_dim=output_dim, h_dim=h_dim)\n",
    "\n",
    "    logits_real = discriminator(X,h_dim=h_dim)\n",
    "    logits_fake = discriminator(X_fake,h_dim=h_dim,reuse=True)\n",
    "\n",
    "    disc_loss = discriminator_loss(logits_real,logits_fake)\n",
    "    gen_loss = generator_loss(logits_fake)\n",
    "\n",
    "    disc_train_op,gen_train_op = training_op(disc_loss, gen_loss, learning_rate=learning_rate, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        d = data(mu1=mu1,mu2=mu2,sig1=sig1,sig2=sig2,batch_size=batch_size) \n",
    "        feed_dict = {X:d}\n",
    "        for i in range(batch_it):\n",
    "            _,disc_cost,_,gen_cost = sess.run([disc_train_op,disc_loss,gen_train_op,gen_loss],feed_dict=feed_dict)\n",
    "            _ = sess.run(gen_train_op)\n",
    "        if epoch %20 == 0:\n",
    "            x_fake = sess.run(X_fake)\n",
    "            print(\"====\")\n",
    "            print(\"Iteration: {}/{}\".format(epoch,epochs-1))\n",
    "            print(\"Discriminator loss: {}\".format(disc_cost))\n",
    "            print(\"Generator loss: {}\".format(gen_cost))\n",
    "            generate_image(epoch,x_fake)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
